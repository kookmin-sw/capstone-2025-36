{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ì‚¬ìš©í•  ë¼ì´ë¸ŒëŸ¬ë¦¬"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "12.4\n",
      "90100\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available())  # Trueì—¬ì•¼ ì •ìƒ\n",
    "print(torch.version.cuda)  # CUDA ë²„ì „ í™•ì¸\n",
    "print(torch.backends.cudnn.version())  # cuDNN ë²„ì „ í™•ì¸\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kookmin/venvs/seungminENV_pip/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸ ì™„ë£Œ\n"
     ]
    }
   ],
   "source": [
    "# ì‚¬ìš©í•  ë¼ì´ë¸ŒëŸ¬ë¦¬\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import json\n",
    "import torch\n",
    "import torchaudio\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from transformers import (\n",
    "    Wav2Vec2Processor,\n",
    "    Wav2Vec2ForCTC,\n",
    "    TrainingArguments,\n",
    "    Trainer\n",
    ")\n",
    "\n",
    "import evaluate\n",
    "\n",
    "print(\"ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸ ì™„ë£Œ\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torchaudio backend set to sox_io\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1326908/2652432896.py:4: UserWarning: torchaudio._backend.set_audio_backend has been deprecated. With dispatcher enabled, this function is no-op. You can remove the function call.\n",
      "  torchaudio.set_audio_backend(\"sox_io\")\n"
     ]
    }
   ],
   "source": [
    "# torchaudio - backend\n",
    "\n",
    "import torchaudio\n",
    "torchaudio.set_audio_backend(\"sox_io\")\n",
    "print(\"torchaudio backend set to sox_io\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "í˜„ì¬ backend: None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1326908/2392935519.py:3: UserWarning: torchaudio._backend.get_audio_backend has been deprecated. With dispatcher enabled, this function is no-op. You can remove the function call.\n",
      "  print(\"í˜„ì¬ backend:\", torchaudio.get_audio_backend())  # ì œê±° ê°€ëŠ¥\n"
     ]
    }
   ],
   "source": [
    "import torchaudio\n",
    "\n",
    "print(\"í˜„ì¬ backend:\", torchaudio.get_audio_backend())  # ì œê±° ê°€ëŠ¥\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch version: 2.6.0+cu124\n",
      "torchaudio version: 2.6.0+cu124\n"
     ]
    }
   ],
   "source": [
    "import torch, torchaudio\n",
    "print(\"torch version:\", torch.__version__)\n",
    "print(\"torchaudio version:\", torchaudio.__version__)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# hyper parameter & ì €ì¥ëœ ë°ì´í„° ê²½ë¡œ & ì €ì¥ ê²½ë¡œ\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì„¤ì • ì™„ë£Œ\n"
     ]
    }
   ],
   "source": [
    "# hyper parameter & ì €ì¥ëœ ë°ì´í„° ê²½ë¡œ & ì €ì¥ ê²½ë¡œ\n",
    "\n",
    "\n",
    "# ë°ì´í„° ê²½ë¡œ\n",
    "DATA_DIR_TRAIN = '/data/seungmin/training'\n",
    "DATA_DIR_VAL = '/data/seungmin/validation'\n",
    "\n",
    "# ì €ì¥ ê²½ë¡œ\n",
    "MODEL_SAVE_DIR = '/data/seungmin/model'\n",
    "\n",
    "# @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@ í•˜ì´í¼íŒŒë¼ë¯¸í„° ì„¤ì • @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@ \n",
    "BATCH_SIZE = 1 # 8ë¡œ í–ˆë”ë‹ˆ cuda out of mem ë–´ìŒ\n",
    "GRAD_ACCUMULATION_STEPS = 16 # 2ë¡œ í–ˆë”ë‹ˆ cuda out of memë–´ìŒ\n",
    "LEARNING_RATE = 1e-5\n",
    "NUM_EPOCHS = 1\n",
    "WARMUP_RATIO = 0.1\n",
    "MAX_DURATION_SEC = 30  # ìµœëŒ€ ì…ë ¥ ê¸¸ì´ 30ì´ˆ\n",
    "SAMPLE_RATE = 16000\n",
    "\n",
    "print(\"ì„¤ì • ì™„ë£Œ\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ë™ì‘ ê°€ëŠ¥í•œì§€ í…ŒìŠ¤íŠ¸í•´ë³´ë ¤ê³  ì‹¤í–‰í•œ ê²ƒì„ ì´ ì…€ì€ ì‹¤í–‰ ã„´ã„´ã„´ã„´\n",
    "## ë°ì´í„°ì…‹ ì¤‘ ì¼ë¶€ë§Œ ì‚¬ìš©"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEBUG_MODE = True\n",
    "# DEBUG_TRAIN_LIMIT = 100   # í›ˆë ¨ ìƒ˜í”Œ ì œí•œ\n",
    "# DEBUG_VAL_LIMIT = 20      # ê²€ì¦ ìƒ˜í”Œ ì œí•œ\n",
    "\n",
    "# # ë°ì´í„°ì…‹ ìƒì„± (íŒŒì¼ ë§¤ì¹­ ë° í•„í„°ë§)\n",
    "# def build_file_list(data_dir, limit=None):\n",
    "#     \"\"\"\n",
    "#     ì£¼ì–´ì§„ data_dir ë‚´ì—ì„œ sourceì˜ wav íŒŒì¼ê³¼ labelsì˜ json íŒŒì¼ì„ ë§¤ì¹­í•˜ì—¬ ë¦¬ìŠ¤íŠ¸ë¡œ ë°˜í™˜.\n",
    "#     - ê° wav íŒŒì¼ì„ torchaudio.load()ë¡œ ë¡œë”© í…ŒìŠ¤íŠ¸í•  ë•Œ, format=\"wav\"ë¥¼ ëª…ì‹œí•˜ì—¬ ì‹œë„í•©ë‹ˆë‹¤.\n",
    "#     - ë¬¸ì œê°€ ë°œìƒí•˜ë©´ fallbackìœ¼ë¡œ soundfileì„ ì‚¬ìš©í•©ë‹ˆë‹¤.\n",
    "#     - ë‘ ë°©ë²• ëª¨ë‘ ì‹¤íŒ¨í•˜ë©´ í•´ë‹¹ íŒŒì¼ì€ ê±´ë„ˆëœë‹ˆë‹¤.\n",
    "#     - limitê°€ ì§€ì •ë˜ë©´ í•´ë‹¹ ê°œìˆ˜ë§Œ ë°˜í™˜í•©ë‹ˆë‹¤.\n",
    "#     \"\"\"\n",
    "#     file_list = []\n",
    "#     audio_pattern = os.path.join(data_dir, 'source', 'eng', '*', '*', '*.wav')\n",
    "#     audio_files = glob.glob(audio_pattern)\n",
    "    \n",
    "#     for audio_path in audio_files:\n",
    "#         json_path = audio_path.replace(os.path.join('source', 'eng'), os.path.join('labels', 'eng')).replace('.wav', '.json')\n",
    "#         if not os.path.exists(json_path):\n",
    "#             print(f\"JSON íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {json_path}\")\n",
    "#             continue\n",
    "        \n",
    "#         loaded = False\n",
    "#         try:\n",
    "#             # format ì¸ìë¥¼ ì¶”ê°€í•´ wav íŒŒì¼ì„ì„ ëª…ì‹œí•©ë‹ˆë‹¤.\n",
    "#             _waveform, sr = torchaudio.load(audio_path, format=\"wav\")\n",
    "#             loaded = True\n",
    "#         except Exception as e:\n",
    "#             print(f\"torchaudio.load() ì‹¤íŒ¨: {audio_path} ì—ì„œ ì˜¤ë¥˜ ë°œìƒ: {e}\")\n",
    "        \n",
    "#         if not loaded:\n",
    "#             try:\n",
    "#                 import soundfile as sf\n",
    "#                 _waveform, sr = sf.read(audio_path)\n",
    "#                 _waveform = torch.tensor(_waveform).float()\n",
    "#                 loaded = True\n",
    "#                 print(f\"soundfile ë¡œë“œ ì„±ê³µ: {audio_path}\")\n",
    "#             except Exception as e:\n",
    "#                 print(f\"soundfile ë¡œë“œ ì‹¤íŒ¨: {audio_path} ì—ì„œ ì˜¤ë¥˜ ë°œìƒ: {e}\")\n",
    "#                 continue\n",
    "        \n",
    "#         try:\n",
    "#             with open(json_path, 'r', encoding='utf-8') as f:\n",
    "#                 data = json.load(f)\n",
    "#             text = data[\"06_transcription\"][\"1_text\"]\n",
    "#         except Exception as e:\n",
    "#             print(f\"JSON íŒŒì¼ ë¡œë“œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {json_path}, {e}\")\n",
    "#             continue\n",
    "        \n",
    "#         file_list.append({\n",
    "#             \"audio_path\": audio_path,\n",
    "#             \"json_path\": json_path,\n",
    "#             \"text\": text\n",
    "#         })\n",
    "        \n",
    "#         if limit is not None and len(file_list) >= limit:\n",
    "#             break\n",
    "#     return file_list\n",
    "\n",
    "\n",
    "# # í›ˆë ¨ ë° ê²€ì¦ ë°ì´í„° íŒŒì¼ ë¦¬ìŠ¤íŠ¸ ìƒì„± (DEBUG_MODEì— ë”°ë¼ ì¼ë¶€ë§Œ ì‚¬ìš©)\n",
    "# train_files = build_file_list(DATA_DIR_TRAIN, limit=DEBUG_TRAIN_LIMIT if DEBUG_MODE else None)\n",
    "# val_files = build_file_list(DATA_DIR_VAL, limit=DEBUG_VAL_LIMIT if DEBUG_MODE else None)\n",
    "\n",
    "# print(\"ì˜ˆì‹œ í›ˆë ¨ íŒŒì¼:\", train_files[0])\n",
    "# print(\"ì˜ˆì‹œ ê²€ì¦ íŒŒì¼:\", val_files[0])\n",
    "# print(f\"í›ˆë ¨ íŒŒì¼ ìˆ˜: {len(train_files)}\")\n",
    "# print(f\"ê²€ì¦ íŒŒì¼ ìˆ˜: {len(val_files)}\")\n",
    "# print(\"DEBUG_MODEê°€ í™œì„±í™”ë˜ì–´ ì¼ë¶€ ìƒ˜í”Œë§Œ ì‚¬ìš©í•©ë‹ˆë‹¤.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ë°ì´í„° íŒŒì¼ ë¦¬ìŠ¤íŠ¸ ìƒì„± (wavì™€ JSON ë§¤ì¹­)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì˜ˆì‹œ í›ˆë ¨ íŒŒì¼: {'audio_path': '/data/seungmin/training/source/eng/comp/C03102/U00414.wav', 'json_path': '/data/seungmin/training/labels/eng/comp/C03102/U00414.json', 'text': 'ë‚®ê³  ê·¸ë‹¤ìŒì— í”„ë¡­ì˜ ì¶”ì§„ë ¥ì´ ì¢€ ì„¸ì•¼ ë˜ ê°•í•´ì•¼ ë˜ê¸° ë•Œë¬¸ì— ì½”ë“œê°€ ê¸´ (sf)/(ì—ìŠ¤ì—í”„) í˜•ìƒ ì´ëŸ° í˜•ìƒì„ ê¸°ì¤€ìœ¼ë¡œ ì„  ì„ íƒí•˜ëŠ”ê²Œ ì¢€ ë” ì¢‹ê³  ì €ì¤‘ëŸ‰ ê°™ì€ ê²½ìš°ì—ëŠ” ì½”ë“œê°€ ì¢€ ì§§ì€ ì´ëŸ° í˜•ìƒì„ ì„ íƒí•˜ëŠ” ê²ƒì´ ì¡°ê¸ˆ ë” ì¢‹ì„ ìˆ˜ ìˆë‹¤ ë³¼ ìˆ˜ ìˆëŠ” ê±°ì˜ˆìš”.'}\n",
      "í›ˆë ¨ íŒŒì¼ ìˆ˜: 357212\n",
      "ê²€ì¦ íŒŒì¼ ìˆ˜: 40387\n"
     ]
    }
   ],
   "source": [
    "# ë°ì´í„° íŒŒì¼ ë¦¬ìŠ¤íŠ¸ ìƒì„± (wavì™€ JSON ë§¤ì¹­)\n",
    "\n",
    "def build_file_list(data_dir):\n",
    "    \"\"\"\n",
    "    ì£¼ì–´ì§„ data_dir ë‚´ì—ì„œ sourceì˜ wav íŒŒì¼ê³¼ labelsì˜ json íŒŒì¼ì„ ë§¤ì¹­í•˜ì—¬ ë¦¬ìŠ¤íŠ¸ë¡œ ë°˜í™˜\n",
    "    \"\"\"\n",
    "    file_list = []\n",
    "    \n",
    "    # wav íŒŒì¼ ê²½ë¡œ: data/seungmin/{training, validation}/source/eng/{ë„ë©”ì¸}/{ê°•ì˜ID}/*.wav\n",
    "    audio_pattern = os.path.join(data_dir, 'source', 'eng', '*', '*', '*.wav')\n",
    "    audio_files = glob.glob(audio_pattern)\n",
    "    \n",
    "    for audio_path in audio_files:\n",
    "        # sourceì—ì„œ labels ê²½ë¡œë¡œ ë³€í™˜ (source -> labels, .wav -> .json)\n",
    "        json_path = audio_path.replace(os.path.join('source', 'eng'), os.path.join('labels', 'eng')).replace('.wav', '.json') # source/eng â†’ labels/eng ë¡œ ë³€í™˜í•˜ê³  & .wav â†’ .json ìœ¼ë¡œ ë³€ê²½í•˜ì—¬ WAV íŒŒì¼ê³¼ ë§¤ì¹­ë˜ëŠ” JSON íŒŒì¼ ê²½ë¡œ ìƒì„±\n",
    "        if os.path.exists(json_path):\n",
    "            try:\n",
    "                with open(json_path, 'r', encoding='utf-8') as f:\n",
    "                    data = json.load(f)\n",
    "                \n",
    "                # JSON ë‚´ \"06_transcription\"ì—ì„œ ì‹¤ì œ ì „ì‚¬ í…ìŠ¤íŠ¸ ì¶”ì¶œ\n",
    "                text = data[\"06_transcription\"][\"1_text\"]\n",
    "                file_list.append({\n",
    "                    \"audio_path\": audio_path,\n",
    "                    \"json_path\": json_path,\n",
    "                    \"text\": text\n",
    "                })\n",
    "            except Exception as e:\n",
    "                print(f\"íŒŒì¼ {json_path} ë¡œë“œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}\")\n",
    "        else:\n",
    "            print(f\"JSON íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŒ : {json_path}\")\n",
    "    return file_list\n",
    "\n",
    "# í›ˆë ¨ ë° ê²€ì¦ ë°ì´í„° íŒŒì¼ ë¦¬ìŠ¤íŠ¸ ìƒì„±\n",
    "train_files = build_file_list(DATA_DIR_TRAIN)\n",
    "val_files = build_file_list(DATA_DIR_VAL)\n",
    "\n",
    "print(\"ì˜ˆì‹œ í›ˆë ¨ íŒŒì¼:\", train_files[0])\n",
    "\n",
    "print(f\"í›ˆë ¨ íŒŒì¼ ìˆ˜: {len(train_files)}\")\n",
    "print(f\"ê²€ì¦ íŒŒì¼ ìˆ˜: {len(val_files)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ì˜¤ë””ì˜¤ ë¡œë”© ë° ì „ì²˜ë¦¬(custom dataset class ì •ì˜)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SpeechDataset class ì •ì˜ ì™„ë£Œ\n"
     ]
    }
   ],
   "source": [
    "class SpeechDataset(Dataset):\n",
    "    def __init__(self, file_list, processor, sample_rate=SAMPLE_RATE, max_duration=MAX_DURATION_SEC):\n",
    "        \"\"\"\n",
    "        file_list: build_file_list()ë¡œ ìƒì„±í•œ ì›ë³¸ íŒŒì¼ ë¦¬ìŠ¤íŠ¸\n",
    "        processor: Wav2Vec2Processor ê°ì²´\n",
    "        sample_rate: ëª©í‘œ ìƒ˜í”Œë§ ë ˆì´íŠ¸\n",
    "        max_duration: ìµœëŒ€ ê¸¸ì´(ì´ˆ)\n",
    "        \n",
    "        ìƒì„±ìì—ì„œ ê° íŒŒì¼ì„ ë¯¸ë¦¬ ë¡œë”© í…ŒìŠ¤íŠ¸í•˜ì—¬ ìœ íš¨í•œ íŒŒì¼ë§Œ í¬í•¨.\n",
    "        \"\"\"\n",
    "        valid_list = []\n",
    "        for item in file_list:\n",
    "            audio_path = item['audio_path']\n",
    "            loaded = False\n",
    "            try:\n",
    "                _waveform, sr = torchaudio.load(audio_path, format=\"wav\")\n",
    "                loaded = True\n",
    "            except Exception as e:\n",
    "                try:\n",
    "                    import soundfile as sf\n",
    "                    waveform, sr = sf.read(audio_path)\n",
    "                    _ = torch.tensor(waveform).float()\n",
    "                    loaded = True\n",
    "                    print(f\"íŒŒì¼ {audio_path}ëŠ” soundfileë¡œ ë¡œë“œ ê°€ëŠ¥í•©ë‹ˆë‹¤.\")\n",
    "                except Exception as e2:\n",
    "                    print(f\"íŒŒì¼ {audio_path} í•„í„°ë§ë¨: torchaudio ì‹¤íŒ¨({e}), soundfile ì‹¤íŒ¨({e2})\")\n",
    "                    continue\n",
    "            valid_list.append(item)\n",
    "        self.file_list = valid_list\n",
    "        self.processor = processor\n",
    "        self.sample_rate = sample_rate\n",
    "        self.max_length = sample_rate * max_duration\n",
    "        print(f\"SpeechDataset ìƒì„± ì™„ë£Œ: {len(self.file_list)}ê°œì˜ ìœ íš¨í•œ ìƒ˜í”Œ í¬í•¨.\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.file_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.file_list[idx]\n",
    "        audio_path = item['audio_path']\n",
    "        loaded = False\n",
    "        try:\n",
    "            speech_tensor, sr = torchaudio.load(audio_path, format=\"wav\")\n",
    "            loaded = True\n",
    "        except Exception as e:\n",
    "            print(f\"__getitem__: torchaudio.load() ì‹¤íŒ¨: {audio_path} ({e})\")\n",
    "        if not loaded:\n",
    "            try:\n",
    "                import soundfile as sf\n",
    "                waveform, sr = sf.read(audio_path)\n",
    "                speech_tensor = torch.tensor(waveform).float()\n",
    "                if speech_tensor.dim() == 1:\n",
    "                    speech_tensor = speech_tensor.unsqueeze(0)\n",
    "                loaded = True\n",
    "                print(f\"__getitem__: soundfile ë¡œë“œ ì„±ê³µ: {audio_path}\")\n",
    "            except Exception as e:\n",
    "                print(f\"__getitem__: íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨: {audio_path} ({e})\")\n",
    "                dummy_audio = np.zeros(self.max_length, dtype=np.float32)\n",
    "                return {\"input_values\": dummy_audio, \"labels\": \"\"}\n",
    "        if sr != self.sample_rate:\n",
    "            resampler = torchaudio.transforms.Resample(orig_freq=sr, new_freq=self.sample_rate)\n",
    "            speech_tensor = resampler(speech_tensor)\n",
    "        speech_array = speech_tensor.squeeze().numpy()\n",
    "        if len(speech_array) > self.max_length:\n",
    "            speech_array = speech_array[:self.max_length]\n",
    "        text = item['text']\n",
    "        # í‚¤ ì´ë¦„ì„ ë³€ê²½í•˜ì—¬ ëª¨ë¸ì˜ ê¸°ëŒ€ì…ë ¥ê³¼ ë§ì¶¤\n",
    "        return {\"input_values\": speech_array, \"labels\": text}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"SpeechDataset class ì •ì˜ ì™„ë£Œ\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# model, processor ë¡œë”©"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ëª¨ë¸ ë° í”„ë¡œì„¸ì„œ ë¡œë“œ ì™„ë£Œ\n"
     ]
    }
   ],
   "source": [
    "# model, processor ë¡œë”©\n",
    "\n",
    "processor = Wav2Vec2Processor.from_pretrained(\"kresnik/wav2vec2-large-xlsr-korean\")\n",
    "model = Wav2Vec2ForCTC.from_pretrained(\"kresnik/wav2vec2-large-xlsr-korean\")\n",
    "\n",
    "print(\"ëª¨ë¸ ë° í”„ë¡œì„¸ì„œ ë¡œë“œ ì™„ë£Œ\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ë°ì´í„°ì…‹ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SpeechDataset ìƒì„± ì™„ë£Œ: 357212ê°œì˜ ìœ íš¨í•œ ìƒ˜í”Œ í¬í•¨.\n",
      "SpeechDataset ìƒì„± ì™„ë£Œ: 40387ê°œì˜ ìœ íš¨í•œ ìƒ˜í”Œ í¬í•¨.\n",
      "{'input_values': array([-0.00125122,  0.00015259, -0.00076294, ...,  0.00091553,\n",
      "       -0.00012207,  0.00088501], shape=(282633,), dtype=float32), 'labels': 'ë‚®ê³  ê·¸ë‹¤ìŒì— í”„ë¡­ì˜ ì¶”ì§„ë ¥ì´ ì¢€ ì„¸ì•¼ ë˜ ê°•í•´ì•¼ ë˜ê¸° ë•Œë¬¸ì— ì½”ë“œê°€ ê¸´ (sf)/(ì—ìŠ¤ì—í”„) í˜•ìƒ ì´ëŸ° í˜•ìƒì„ ê¸°ì¤€ìœ¼ë¡œ ì„  ì„ íƒí•˜ëŠ”ê²Œ ì¢€ ë” ì¢‹ê³  ì €ì¤‘ëŸ‰ ê°™ì€ ê²½ìš°ì—ëŠ” ì½”ë“œê°€ ì¢€ ì§§ì€ ì´ëŸ° í˜•ìƒì„ ì„ íƒí•˜ëŠ” ê²ƒì´ ì¡°ê¸ˆ ë” ì¢‹ì„ ìˆ˜ ìˆë‹¤ ë³¼ ìˆ˜ ìˆëŠ” ê±°ì˜ˆìš”.'}\n",
      "í›ˆë ¨ ë°ì´í„°ì…‹ ìƒ˜í”Œ ìˆ˜: 357212\n",
      "ê²€ì¦ ë°ì´í„°ì…‹ ìƒ˜í”Œ ìˆ˜: 40387\n"
     ]
    }
   ],
   "source": [
    "# ë°ì´í„°ì…‹ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±\n",
    "\n",
    "train_dataset = SpeechDataset(train_files, processor, sample_rate=SAMPLE_RATE, max_duration=MAX_DURATION_SEC)\n",
    "val_dataset = SpeechDataset(val_files, processor, sample_rate=SAMPLE_RATE, max_duration=MAX_DURATION_SEC)\n",
    "\n",
    "print(train_dataset[0])\n",
    "print(f\"í›ˆë ¨ ë°ì´í„°ì…‹ ìƒ˜í”Œ ìˆ˜: {len(train_dataset)}\")\n",
    "print(f\"ê²€ì¦ ë°ì´í„°ì…‹ ìƒ˜í”Œ ìˆ˜: {len(val_dataset)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ë¹„ì–´ ìˆëŠ” ìƒ˜í”Œ ê°œìˆ˜: 0\n"
     ]
    }
   ],
   "source": [
    "# only ë””ë²„ê¹…ìš©\n",
    "\n",
    "\n",
    "# ë¹ˆ ìƒ˜í”Œì´ ìˆëŠ”ì§€ í™•ì¸\n",
    "empty_samples = [i for i, sample in enumerate(train_dataset) if len(sample[\"input_values\"]) == 0 or sample[\"labels\"] == \"\"]\n",
    "print(f\"ë¹„ì–´ ìˆëŠ” ìƒ˜í”Œ ê°œìˆ˜: {len(empty_samples)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ë°ì´í„° collator ì •ì˜ (dynamic padding í¬í•¨)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ë°ì´í„° collator ì •ì˜ (dynamic padding í¬í•¨)\n",
    "\n",
    "# # 'ë¹„ì–´ ìˆëŠ” ìƒ˜í”Œ ê°œìˆ˜: 0'ë¼ê³  ë‚˜ì˜¤ëŠ”ë°, ìê¾¸ íŒŒì¸íŠœë‹ ì½”ë“œ ì‹¤í–‰ ì‹œ 'ë°°ì¹˜ì˜ ìƒ˜í”Œì´ ìœ íš¨í•˜ì§€ ì•Šë‹¤'ê³  ì¶œë ¥ë˜ì–´ì„œ ë””ë²„ê¹… ì½”ë“œ ì¶”ê°€í•˜ì˜€ìŒ\n",
    "\n",
    "# def data_collator(batch):\n",
    "#     # ë°°ì¹˜ê°€ ë”•ì…”ë„ˆë¦¬ í˜•íƒœì´ë©´ ì´ë¯¸ collateëœ ìƒíƒœì„\n",
    "#     if isinstance(batch, dict):\n",
    "#         speeches = batch.get(\"speech\", [])\n",
    "#         texts = batch.get(\"text\", [])\n",
    "#     else:\n",
    "#         # ë¦¬ìŠ¤íŠ¸ì¸ ê²½ìš° ê°œë³„ ìƒ˜í”Œì„ í•„í„°ë§\n",
    "#         valid_batch = [\n",
    "#             item for item in batch \n",
    "#             if item and \"speech\" in item and \"text\" in item and len(item[\"speech\"]) > 0 and item[\"text\"]\n",
    "#         ]\n",
    "#         if len(valid_batch) == 0:\n",
    "#             raise ValueError(\"ëª¨ë“  ë°°ì¹˜ì˜ ìƒ˜í”Œì´ ìœ íš¨í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.\")\n",
    "#         speeches = [item[\"speech\"] for item in valid_batch]\n",
    "#         texts = [item[\"text\"] for item in valid_batch]\n",
    "\n",
    "#     print(f\"ğŸ“Œ ìµœì¢… ë°°ì¹˜ í¬ê¸°: {len(speeches)}\")\n",
    "#     print(f\" ì²« ë²ˆì§¸ wav ê¸¸ì´: {len(speeches[0]) if speeches else 'N/A'}\")\n",
    "#     print(f\" ì²« ë²ˆì§¸ í…ìŠ¤íŠ¸: {texts[0] if texts else 'N/A'}\")\n",
    "    \n",
    "#     inputs = processor(speeches, text=texts, sampling_rate=SAMPLE_RATE, padding=True, return_tensors=\"pt\")\n",
    "#     return inputs\n",
    "\n",
    "\n",
    "\n",
    "# print(\"data_collator ì •ì˜ ì™„ë£Œ\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_collator(features):\n",
    "    # featuresëŠ” {\"input_values\": ..., \"labels\": ...} í˜•íƒœì˜ ë”•ì…”ë„ˆë¦¬ ë¦¬ìŠ¤íŠ¸ì…ë‹ˆë‹¤.\n",
    "    if isinstance(features, list):\n",
    "        if features and isinstance(features[0], dict):\n",
    "            # \"input_values\" í‚¤ê°€ ìˆëŠ”ì§€ í™•ì¸í•©ë‹ˆë‹¤.\n",
    "            if \"input_values\" in features[0]:\n",
    "                input_values = [f[\"input_values\"] for f in features]\n",
    "                labels = [f[\"labels\"] for f in features]\n",
    "            else:\n",
    "                raise ValueError(\"í‚¤ 'input_values'ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.\")\n",
    "        else:\n",
    "            raise ValueError(\"ë¦¬ìŠ¤íŠ¸ ë‚´ ìš”ì†Œê°€ dict í˜•íƒœê°€ ì•„ë‹™ë‹ˆë‹¤.\")\n",
    "    elif isinstance(features, dict):\n",
    "        if \"input_values\" in features:\n",
    "            input_values = features.get(\"input_values\", [])\n",
    "            labels = features.get(\"labels\", [])\n",
    "        else:\n",
    "            raise ValueError(\"dict í˜•íƒœì§€ë§Œ 'input_values' í‚¤ê°€ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "    else:\n",
    "        raise ValueError(\"ë°°ì¹˜ì˜ í˜•ì‹ì´ ì˜¬ë°”ë¥´ì§€ ì•ŠìŠµë‹ˆë‹¤.\")\n",
    "    \n",
    "    # processorë¥¼ ì‚¬ìš©í•´ ë™ì  íŒ¨ë”© ìˆ˜í–‰\n",
    "    batch = processor(input_values, text=labels, sampling_rate=SAMPLE_RATE, padding=True, return_tensors=\"pt\")\n",
    "    return batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ëª¨ë¸, í”„ë¡œì„¸ì„œ ë¡œë”© ì´í›„ì— gradient checkpointing í™œì„±í™”\n",
    "model.gradient_checkpointing_enable()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training aruments ì„¤ì •"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TrainingArguments ì„¤ì • ì™„ë£Œ\n"
     ]
    }
   ],
   "source": [
    "# Training arguments ì„¤ì •ì •\n",
    "\n",
    "# í•˜ì´í¼íŒŒë¼ë¯¸í„° ì„¤ì •ì€ ìœ„ì—ì„œ í•˜ê¸° \n",
    "training_args = TrainingArguments(\n",
    "    output_dir=MODEL_SAVE_DIR,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=BATCH_SIZE,\n",
    "    gradient_accumulation_steps=GRAD_ACCUMULATION_STEPS,\n",
    "    \n",
    "    eval_strategy=\"epoch\", # evaluation_strategyëŠ” êµ¬ë²„ì ¼ì—ì„œë§Œ ì˜ë¨\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    num_train_epochs=NUM_EPOCHS,\n",
    "    warmup_ratio=WARMUP_RATIO,\n",
    "    fp16=True, # mixed precision\n",
    "    logging_steps=100,\n",
    "    save_total_limit=2,  # best ê²°ê³¼ + latest ê²°ê³¼ë§Œ ì €ì¥\n",
    "    load_best_model_at_end=True,\n",
    "    dataloader_num_workers=0\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "print(\"TrainingArguments ì„¤ì • ì™„ë£Œ\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# í‰ê°€ ì§€í‘œ í•¨ìˆ˜ ì •ì˜ (WER ê¸°ë°˜)\n",
    "evaluation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluation metric í•¨ìˆ˜ ì •ì˜ ì™„ë£Œ\n"
     ]
    }
   ],
   "source": [
    "# í‰ê°€ ì§€í‘œ í•¨ìˆ˜ ì •ì˜ (WER ê¸°ë°˜)\n",
    "\n",
    "wer_metric = evaluate.load(\"wer\") # evaluate ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì‚¬ìš©í•´ WER metric ë¡œë“œ\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    \"\"\"\n",
    "        ì˜ˆì¸¡ ê²°ê³¼ì™€ ë¼ë²¨ì„ processorë¥¼ ì‚¬ìš©í•˜ì—¬ ë””ì½”ë”© í›„ WER ê³„ì‚°\n",
    "    \"\"\"\n",
    "    pred_logits = pred.predictions\n",
    "    pred_ids = np.argmax(pred_logits, axis=-1)\n",
    "    pred_str = processor.batch_decode(pred_ids)\n",
    "    \n",
    "    # -100ì€ ë¬´ì‹œ ì²˜ë¦¬: label_ids ë‚´ì˜ ê°’ë“¤ì„ ë””ì½”ë”© \n",
    "    label_ids = pred.label_ids\n",
    "    label_str = processor.batch_decode(label_ids, group_tokens=False) # group_tokens=False ì˜µì…˜ì´ ë­”ì§€ ì˜ ëª¨ë¦„\n",
    "    \n",
    "    wer = wer_metric.compute(predictions=pred_str, references=label_str) # prediction vs label\n",
    "    return {\"wer\": wer}\n",
    "\n",
    "print(\"evaluation metric í•¨ìˆ˜ ì •ì˜ ì™„ë£Œ\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì²« ë²ˆì§¸ ìƒ˜í”Œ speech ê¸¸ì´: 282633\n",
      "ì²« ë²ˆì§¸ ìƒ˜í”Œ text: ë‚®ê³  ê·¸ë‹¤ìŒì— í”„ë¡­ì˜ ì¶”ì§„ë ¥ì´ ì¢€ ì„¸ì•¼ ë˜ ê°•í•´ì•¼ ë˜ê¸° ë•Œë¬¸ì— ì½”ë“œê°€ ê¸´ (sf)/(ì—ìŠ¤ì—í”„) í˜•ìƒ ì´ëŸ° í˜•ìƒì„ ê¸°ì¤€ìœ¼ë¡œ ì„  ì„ íƒí•˜ëŠ”ê²Œ ì¢€ ë” ì¢‹ê³  ì €ì¤‘ëŸ‰ ê°™ì€ ê²½ìš°ì—ëŠ” ì½”ë“œê°€ ì¢€ ì§§ì€ ì´ëŸ° í˜•ìƒì„ ì„ íƒí•˜ëŠ” ê²ƒì´ ì¡°ê¸ˆ ë” ì¢‹ì„ ìˆ˜ ìˆë‹¤ ë³¼ ìˆ˜ ìˆëŠ” ê±°ì˜ˆìš”.\n"
     ]
    }
   ],
   "source": [
    "# # ë°ì´í„°ì…‹ í™•ì¸\n",
    "# for i in range(len(train_dataset)):\n",
    "#     sample = train_dataset[i]\n",
    "#     if sample is None:\n",
    "#         print(f\"ì¸ë±ìŠ¤ {i}ëŠ” ìœ íš¨í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.\")\n",
    "\n",
    "\n",
    "# empty_samples = [i for i, sample in enumerate(train_dataset) if not sample[\"speech\"] or not sample[\"text\"]]\n",
    "# print(f\"ë¹„ì–´ ìˆëŠ” ìƒ˜í”Œ ê°œìˆ˜: {len(empty_samples)}\")\n",
    "\n",
    "sample = train_dataset[0]\n",
    "print(f\"ì²« ë²ˆì§¸ ìƒ˜í”Œ speech ê¸¸ì´: {len(sample['input_values'])}\")\n",
    "print(f\"ì²« ë²ˆì§¸ ìƒ˜í”Œ text: {sample['labels']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trainer ê°ì²´ ìƒì„±  & í•™ìŠµ ì‹¤í–‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainer ê°ì²´ ìƒì„± ì™„ë£Œ\n",
      "íŒŒì¸íŠœë‹ ì‹œì‘\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='6183' max='22325' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 6183/22325 4:54:24 < 12:48:52, 0.35 it/s, Epoch 0.28/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[23]\u001b[39m\u001b[32m, line 19\u001b[39m\n\u001b[32m     15\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mTrainer ê°ì²´ ìƒì„± ì™„ë£Œ\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     16\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33míŒŒì¸íŠœë‹ ì‹œì‘\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m19\u001b[39m train_result = \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     20\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33míŒŒì¸íŠœë‹ ì™„ë£Œ\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/venvs/seungminENV_pip/lib/python3.12/site-packages/transformers/trainer.py:2241\u001b[39m, in \u001b[36mTrainer.train\u001b[39m\u001b[34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[39m\n\u001b[32m   2239\u001b[39m         hf_hub_utils.enable_progress_bars()\n\u001b[32m   2240\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2241\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2242\u001b[39m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2243\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2244\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2245\u001b[39m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2246\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/venvs/seungminENV_pip/lib/python3.12/site-packages/transformers/trainer.py:2548\u001b[39m, in \u001b[36mTrainer._inner_training_loop\u001b[39m\u001b[34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[39m\n\u001b[32m   2541\u001b[39m context = (\n\u001b[32m   2542\u001b[39m     functools.partial(\u001b[38;5;28mself\u001b[39m.accelerator.no_sync, model=model)\n\u001b[32m   2543\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m i != \u001b[38;5;28mlen\u001b[39m(batch_samples) - \u001b[32m1\u001b[39m\n\u001b[32m   2544\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.accelerator.distributed_type != DistributedType.DEEPSPEED\n\u001b[32m   2545\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m contextlib.nullcontext\n\u001b[32m   2546\u001b[39m )\n\u001b[32m   2547\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[32m-> \u001b[39m\u001b[32m2548\u001b[39m     tr_loss_step = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2550\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m   2551\u001b[39m     args.logging_nan_inf_filter\n\u001b[32m   2552\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[32m   2553\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m (torch.isnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch.isinf(tr_loss_step))\n\u001b[32m   2554\u001b[39m ):\n\u001b[32m   2555\u001b[39m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[32m   2556\u001b[39m     tr_loss = tr_loss + tr_loss / (\u001b[32m1\u001b[39m + \u001b[38;5;28mself\u001b[39m.state.global_step - \u001b[38;5;28mself\u001b[39m._globalstep_last_logged)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/venvs/seungminENV_pip/lib/python3.12/site-packages/transformers/trainer.py:3740\u001b[39m, in \u001b[36mTrainer.training_step\u001b[39m\u001b[34m(***failed resolving arguments***)\u001b[39m\n\u001b[32m   3737\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.accelerator.distributed_type == DistributedType.DEEPSPEED:\n\u001b[32m   3738\u001b[39m     kwargs[\u001b[33m\"\u001b[39m\u001b[33mscale_wrt_gas\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m3740\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43maccelerator\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3742\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m loss.detach()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/venvs/seungminENV_pip/lib/python3.12/site-packages/accelerate/accelerator.py:2325\u001b[39m, in \u001b[36mAccelerator.backward\u001b[39m\u001b[34m(self, loss, **kwargs)\u001b[39m\n\u001b[32m   2323\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m   2324\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.scaler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2325\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mscaler\u001b[49m\u001b[43m.\u001b[49m\u001b[43mscale\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2326\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m learning_rate \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.has_lomo_optimizer:\n\u001b[32m   2327\u001b[39m     \u001b[38;5;28mself\u001b[39m.lomo_backward(loss, learning_rate)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/venvs/seungminENV_pip/lib/python3.12/site-packages/torch/_tensor.py:626\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    616\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    617\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    618\u001b[39m         Tensor.backward,\n\u001b[32m    619\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    624\u001b[39m         inputs=inputs,\n\u001b[32m    625\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m626\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    627\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    628\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/venvs/seungminENV_pip/lib/python3.12/site-packages/torch/autograd/__init__.py:347\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    342\u001b[39m     retain_graph = create_graph\n\u001b[32m    344\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    345\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    346\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m347\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    348\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    349\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    350\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    351\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    352\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    353\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    354\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/venvs/seungminENV_pip/lib/python3.12/site-packages/torch/autograd/graph.py:823\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    821\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    822\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m823\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    824\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    825\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    826\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    827\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mí˜„ì¬ ì…€ ë˜ëŠ” ì´ì „ ì…€ì—ì„œ ì½”ë“œë¥¼ ì‹¤í–‰í•˜ëŠ” ë™ì•ˆ Kernelì´ ì¶©ëŒí–ˆìŠµë‹ˆë‹¤. \n",
      "\u001b[1;31mì…€ì˜ ì½”ë“œë¥¼ ê²€í† í•˜ì—¬ ê°€ëŠ¥í•œ ì˜¤ë¥˜ ì›ì¸ì„ ì‹ë³„í•˜ì„¸ìš”. \n",
      "\u001b[1;31mìì„¸í•œ ë‚´ìš©ì„ ë³´ë ¤ë©´ <a href='https://aka.ms/vscodeJupyterKernelCrash'>ì—¬ê¸°</a>ë¥¼ í´ë¦­í•˜ì„¸ìš”. \n",
      "\u001b[1;31mìì„¸í•œ ë‚´ìš©ì€ Jupyter <a href='command:jupyter.viewOutput'>ë¡œê·¸</a>ë¥¼ ì°¸ì¡°í•˜ì„¸ìš”."
     ]
    }
   ],
   "source": [
    "# Trainer ê°ì²´ ìƒì„±  & í•™ìŠµ ì‹¤í–‰\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset\n",
    "    # tokenizer=processor\n",
    "    # dataloader_pin_memory=False # ìµœì‹ ë²„ì ¼ì—ì„œëŠ” ë¯¸ì§€ì›ì´ë¼\n",
    "    # remove_unused_columns=False\n",
    ")\n",
    "\n",
    "print(\"Trainer ê°ì²´ ìƒì„± ì™„ë£Œ\")\n",
    "print(\"íŒŒì¸íŠœë‹ ì‹œì‘\")\n",
    "\n",
    "\n",
    "train_result = trainer.train()\n",
    "print(\"íŒŒì¸íŠœë‹ ì™„ë£Œ\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ëª¨ë¸ ì €ì¥"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ëª¨ë¸ ì €ì¥\n",
    "\n",
    "# ìµœì¢… ëª¨ë¸ê³¼ í”„ë¡œì„¸ì„œ ì €ì¥ (.pt í˜•ì‹ì€ ì•„ë‹ˆì§€ë§Œ Hugging Face í¬ë§·ìœ¼ë¡œ ì €ì¥ í›„ ë³€í™˜ ê°€ëŠ¥)\n",
    "model.save_pretrained(MODEL_SAVE_DIR)\n",
    "processor.save_pretrained(MODEL_SAVE_DIR)\n",
    "\n",
    "print(f\"ëª¨ë¸ì´ {MODEL_SAVE_DIR}ì— ì €ì¥ë¨\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "seungminENV_pip",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
