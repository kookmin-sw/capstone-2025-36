{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 사용할 라이브러리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "12.4\n",
      "90100\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available())  # True여야 정상\n",
    "print(torch.version.cuda)  # CUDA 버전 확인\n",
    "print(torch.backends.cudnn.version())  # cuDNN 버전 확인\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kookmin/venvs/seungminENV_pip/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "라이브러리 임포트 완료\n"
     ]
    }
   ],
   "source": [
    "# 사용할 라이브러리\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import json\n",
    "import torch\n",
    "import torchaudio\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from transformers import (\n",
    "    Wav2Vec2Processor,\n",
    "    Wav2Vec2ForCTC,\n",
    "    TrainingArguments,\n",
    "    Trainer\n",
    ")\n",
    "\n",
    "import evaluate\n",
    "\n",
    "print(\"라이브러리 임포트 완료\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torchaudio backend set to sox_io\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1326908/2652432896.py:4: UserWarning: torchaudio._backend.set_audio_backend has been deprecated. With dispatcher enabled, this function is no-op. You can remove the function call.\n",
      "  torchaudio.set_audio_backend(\"sox_io\")\n"
     ]
    }
   ],
   "source": [
    "# torchaudio - backend\n",
    "\n",
    "import torchaudio\n",
    "torchaudio.set_audio_backend(\"sox_io\")\n",
    "print(\"torchaudio backend set to sox_io\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "현재 backend: None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1326908/2392935519.py:3: UserWarning: torchaudio._backend.get_audio_backend has been deprecated. With dispatcher enabled, this function is no-op. You can remove the function call.\n",
      "  print(\"현재 backend:\", torchaudio.get_audio_backend())  # 제거 가능\n"
     ]
    }
   ],
   "source": [
    "import torchaudio\n",
    "\n",
    "print(\"현재 backend:\", torchaudio.get_audio_backend())  # 제거 가능\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch version: 2.6.0+cu124\n",
      "torchaudio version: 2.6.0+cu124\n"
     ]
    }
   ],
   "source": [
    "import torch, torchaudio\n",
    "print(\"torch version:\", torch.__version__)\n",
    "print(\"torchaudio version:\", torchaudio.__version__)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# hyper parameter & 저장된 데이터 경로 & 저장 경로\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "설정 완료\n"
     ]
    }
   ],
   "source": [
    "# hyper parameter & 저장된 데이터 경로 & 저장 경로\n",
    "\n",
    "\n",
    "# 데이터 경로\n",
    "DATA_DIR_TRAIN = '/data/seungmin/training'\n",
    "DATA_DIR_VAL = '/data/seungmin/validation'\n",
    "\n",
    "# 저장 경로\n",
    "MODEL_SAVE_DIR = '/data/seungmin/model'\n",
    "\n",
    "# @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@ 하이퍼파라미터 설정 @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@ \n",
    "BATCH_SIZE = 1 # 8로 했더니 cuda out of mem 떴음\n",
    "GRAD_ACCUMULATION_STEPS = 16 # 2로 했더니 cuda out of mem떴음\n",
    "LEARNING_RATE = 1e-5\n",
    "NUM_EPOCHS = 1\n",
    "WARMUP_RATIO = 0.1\n",
    "MAX_DURATION_SEC = 30  # 최대 입력 길이 30초\n",
    "SAMPLE_RATE = 16000\n",
    "\n",
    "print(\"설정 완료\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 동작 가능한지 테스트해보려고 실행한 것임 이 셀은 실행 ㄴㄴㄴㄴ\n",
    "## 데이터셋 중 일부만 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEBUG_MODE = True\n",
    "# DEBUG_TRAIN_LIMIT = 100   # 훈련 샘플 제한\n",
    "# DEBUG_VAL_LIMIT = 20      # 검증 샘플 제한\n",
    "\n",
    "# # 데이터셋 생성 (파일 매칭 및 필터링)\n",
    "# def build_file_list(data_dir, limit=None):\n",
    "#     \"\"\"\n",
    "#     주어진 data_dir 내에서 source의 wav 파일과 labels의 json 파일을 매칭하여 리스트로 반환.\n",
    "#     - 각 wav 파일을 torchaudio.load()로 로딩 테스트할 때, format=\"wav\"를 명시하여 시도합니다.\n",
    "#     - 문제가 발생하면 fallback으로 soundfile을 사용합니다.\n",
    "#     - 두 방법 모두 실패하면 해당 파일은 건너뜁니다.\n",
    "#     - limit가 지정되면 해당 개수만 반환합니다.\n",
    "#     \"\"\"\n",
    "#     file_list = []\n",
    "#     audio_pattern = os.path.join(data_dir, 'source', 'eng', '*', '*', '*.wav')\n",
    "#     audio_files = glob.glob(audio_pattern)\n",
    "    \n",
    "#     for audio_path in audio_files:\n",
    "#         json_path = audio_path.replace(os.path.join('source', 'eng'), os.path.join('labels', 'eng')).replace('.wav', '.json')\n",
    "#         if not os.path.exists(json_path):\n",
    "#             print(f\"JSON 파일이 존재하지 않습니다: {json_path}\")\n",
    "#             continue\n",
    "        \n",
    "#         loaded = False\n",
    "#         try:\n",
    "#             # format 인자를 추가해 wav 파일임을 명시합니다.\n",
    "#             _waveform, sr = torchaudio.load(audio_path, format=\"wav\")\n",
    "#             loaded = True\n",
    "#         except Exception as e:\n",
    "#             print(f\"torchaudio.load() 실패: {audio_path} 에서 오류 발생: {e}\")\n",
    "        \n",
    "#         if not loaded:\n",
    "#             try:\n",
    "#                 import soundfile as sf\n",
    "#                 _waveform, sr = sf.read(audio_path)\n",
    "#                 _waveform = torch.tensor(_waveform).float()\n",
    "#                 loaded = True\n",
    "#                 print(f\"soundfile 로드 성공: {audio_path}\")\n",
    "#             except Exception as e:\n",
    "#                 print(f\"soundfile 로드 실패: {audio_path} 에서 오류 발생: {e}\")\n",
    "#                 continue\n",
    "        \n",
    "#         try:\n",
    "#             with open(json_path, 'r', encoding='utf-8') as f:\n",
    "#                 data = json.load(f)\n",
    "#             text = data[\"06_transcription\"][\"1_text\"]\n",
    "#         except Exception as e:\n",
    "#             print(f\"JSON 파일 로드 중 오류 발생: {json_path}, {e}\")\n",
    "#             continue\n",
    "        \n",
    "#         file_list.append({\n",
    "#             \"audio_path\": audio_path,\n",
    "#             \"json_path\": json_path,\n",
    "#             \"text\": text\n",
    "#         })\n",
    "        \n",
    "#         if limit is not None and len(file_list) >= limit:\n",
    "#             break\n",
    "#     return file_list\n",
    "\n",
    "\n",
    "# # 훈련 및 검증 데이터 파일 리스트 생성 (DEBUG_MODE에 따라 일부만 사용)\n",
    "# train_files = build_file_list(DATA_DIR_TRAIN, limit=DEBUG_TRAIN_LIMIT if DEBUG_MODE else None)\n",
    "# val_files = build_file_list(DATA_DIR_VAL, limit=DEBUG_VAL_LIMIT if DEBUG_MODE else None)\n",
    "\n",
    "# print(\"예시 훈련 파일:\", train_files[0])\n",
    "# print(\"예시 검증 파일:\", val_files[0])\n",
    "# print(f\"훈련 파일 수: {len(train_files)}\")\n",
    "# print(f\"검증 파일 수: {len(val_files)}\")\n",
    "# print(\"DEBUG_MODE가 활성화되어 일부 샘플만 사용합니다.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 데이터 파일 리스트 생성 (wav와 JSON 매칭)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "예시 훈련 파일: {'audio_path': '/data/seungmin/training/source/eng/comp/C03102/U00414.wav', 'json_path': '/data/seungmin/training/labels/eng/comp/C03102/U00414.json', 'text': '낮고 그다음에 프롭의 추진력이 좀 세야 되 강해야 되기 때문에 코드가 긴 (sf)/(에스에프) 형상 이런 형상을 기준으로 선 선택하는게 좀 더 좋고 저중량 같은 경우에는 코드가 좀 짧은 이런 형상을 선택하는 것이 조금 더 좋을 수 있다 볼 수 있는 거예요.'}\n",
      "훈련 파일 수: 357212\n",
      "검증 파일 수: 40387\n"
     ]
    }
   ],
   "source": [
    "# 데이터 파일 리스트 생성 (wav와 JSON 매칭)\n",
    "\n",
    "def build_file_list(data_dir):\n",
    "    \"\"\"\n",
    "    주어진 data_dir 내에서 source의 wav 파일과 labels의 json 파일을 매칭하여 리스트로 반환\n",
    "    \"\"\"\n",
    "    file_list = []\n",
    "    \n",
    "    # wav 파일 경로: data/seungmin/{training, validation}/source/eng/{도메인}/{강의ID}/*.wav\n",
    "    audio_pattern = os.path.join(data_dir, 'source', 'eng', '*', '*', '*.wav')\n",
    "    audio_files = glob.glob(audio_pattern)\n",
    "    \n",
    "    for audio_path in audio_files:\n",
    "        # source에서 labels 경로로 변환 (source -> labels, .wav -> .json)\n",
    "        json_path = audio_path.replace(os.path.join('source', 'eng'), os.path.join('labels', 'eng')).replace('.wav', '.json') # source/eng → labels/eng 로 변환하고 & .wav → .json 으로 변경하여 WAV 파일과 매칭되는 JSON 파일 경로 생성\n",
    "        if os.path.exists(json_path):\n",
    "            try:\n",
    "                with open(json_path, 'r', encoding='utf-8') as f:\n",
    "                    data = json.load(f)\n",
    "                \n",
    "                # JSON 내 \"06_transcription\"에서 실제 전사 텍스트 추출\n",
    "                text = data[\"06_transcription\"][\"1_text\"]\n",
    "                file_list.append({\n",
    "                    \"audio_path\": audio_path,\n",
    "                    \"json_path\": json_path,\n",
    "                    \"text\": text\n",
    "                })\n",
    "            except Exception as e:\n",
    "                print(f\"파일 {json_path} 로드 중 오류 발생: {e}\")\n",
    "        else:\n",
    "            print(f\"JSON 파일이 존재하지 않음 : {json_path}\")\n",
    "    return file_list\n",
    "\n",
    "# 훈련 및 검증 데이터 파일 리스트 생성\n",
    "train_files = build_file_list(DATA_DIR_TRAIN)\n",
    "val_files = build_file_list(DATA_DIR_VAL)\n",
    "\n",
    "print(\"예시 훈련 파일:\", train_files[0])\n",
    "\n",
    "print(f\"훈련 파일 수: {len(train_files)}\")\n",
    "print(f\"검증 파일 수: {len(val_files)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 오디오 로딩 및 전처리(custom dataset class 정의)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SpeechDataset class 정의 완료\n"
     ]
    }
   ],
   "source": [
    "class SpeechDataset(Dataset):\n",
    "    def __init__(self, file_list, processor, sample_rate=SAMPLE_RATE, max_duration=MAX_DURATION_SEC):\n",
    "        \"\"\"\n",
    "        file_list: build_file_list()로 생성한 원본 파일 리스트\n",
    "        processor: Wav2Vec2Processor 객체\n",
    "        sample_rate: 목표 샘플링 레이트\n",
    "        max_duration: 최대 길이(초)\n",
    "        \n",
    "        생성자에서 각 파일을 미리 로딩 테스트하여 유효한 파일만 포함.\n",
    "        \"\"\"\n",
    "        valid_list = []\n",
    "        for item in file_list:\n",
    "            audio_path = item['audio_path']\n",
    "            loaded = False\n",
    "            try:\n",
    "                _waveform, sr = torchaudio.load(audio_path, format=\"wav\")\n",
    "                loaded = True\n",
    "            except Exception as e:\n",
    "                try:\n",
    "                    import soundfile as sf\n",
    "                    waveform, sr = sf.read(audio_path)\n",
    "                    _ = torch.tensor(waveform).float()\n",
    "                    loaded = True\n",
    "                    print(f\"파일 {audio_path}는 soundfile로 로드 가능합니다.\")\n",
    "                except Exception as e2:\n",
    "                    print(f\"파일 {audio_path} 필터링됨: torchaudio 실패({e}), soundfile 실패({e2})\")\n",
    "                    continue\n",
    "            valid_list.append(item)\n",
    "        self.file_list = valid_list\n",
    "        self.processor = processor\n",
    "        self.sample_rate = sample_rate\n",
    "        self.max_length = sample_rate * max_duration\n",
    "        print(f\"SpeechDataset 생성 완료: {len(self.file_list)}개의 유효한 샘플 포함.\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.file_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.file_list[idx]\n",
    "        audio_path = item['audio_path']\n",
    "        loaded = False\n",
    "        try:\n",
    "            speech_tensor, sr = torchaudio.load(audio_path, format=\"wav\")\n",
    "            loaded = True\n",
    "        except Exception as e:\n",
    "            print(f\"__getitem__: torchaudio.load() 실패: {audio_path} ({e})\")\n",
    "        if not loaded:\n",
    "            try:\n",
    "                import soundfile as sf\n",
    "                waveform, sr = sf.read(audio_path)\n",
    "                speech_tensor = torch.tensor(waveform).float()\n",
    "                if speech_tensor.dim() == 1:\n",
    "                    speech_tensor = speech_tensor.unsqueeze(0)\n",
    "                loaded = True\n",
    "                print(f\"__getitem__: soundfile 로드 성공: {audio_path}\")\n",
    "            except Exception as e:\n",
    "                print(f\"__getitem__: 파일 로드 실패: {audio_path} ({e})\")\n",
    "                dummy_audio = np.zeros(self.max_length, dtype=np.float32)\n",
    "                return {\"input_values\": dummy_audio, \"labels\": \"\"}\n",
    "        if sr != self.sample_rate:\n",
    "            resampler = torchaudio.transforms.Resample(orig_freq=sr, new_freq=self.sample_rate)\n",
    "            speech_tensor = resampler(speech_tensor)\n",
    "        speech_array = speech_tensor.squeeze().numpy()\n",
    "        if len(speech_array) > self.max_length:\n",
    "            speech_array = speech_array[:self.max_length]\n",
    "        text = item['text']\n",
    "        # 키 이름을 변경하여 모델의 기대입력과 맞춤\n",
    "        return {\"input_values\": speech_array, \"labels\": text}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"SpeechDataset class 정의 완료\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# model, processor 로딩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "모델 및 프로세서 로드 완료\n"
     ]
    }
   ],
   "source": [
    "# model, processor 로딩\n",
    "\n",
    "processor = Wav2Vec2Processor.from_pretrained(\"kresnik/wav2vec2-large-xlsr-korean\")\n",
    "model = Wav2Vec2ForCTC.from_pretrained(\"kresnik/wav2vec2-large-xlsr-korean\")\n",
    "\n",
    "print(\"모델 및 프로세서 로드 완료\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 데이터셋 인스턴스 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SpeechDataset 생성 완료: 357212개의 유효한 샘플 포함.\n",
      "SpeechDataset 생성 완료: 40387개의 유효한 샘플 포함.\n",
      "{'input_values': array([-0.00125122,  0.00015259, -0.00076294, ...,  0.00091553,\n",
      "       -0.00012207,  0.00088501], shape=(282633,), dtype=float32), 'labels': '낮고 그다음에 프롭의 추진력이 좀 세야 되 강해야 되기 때문에 코드가 긴 (sf)/(에스에프) 형상 이런 형상을 기준으로 선 선택하는게 좀 더 좋고 저중량 같은 경우에는 코드가 좀 짧은 이런 형상을 선택하는 것이 조금 더 좋을 수 있다 볼 수 있는 거예요.'}\n",
      "훈련 데이터셋 샘플 수: 357212\n",
      "검증 데이터셋 샘플 수: 40387\n"
     ]
    }
   ],
   "source": [
    "# 데이터셋 인스턴스 생성\n",
    "\n",
    "train_dataset = SpeechDataset(train_files, processor, sample_rate=SAMPLE_RATE, max_duration=MAX_DURATION_SEC)\n",
    "val_dataset = SpeechDataset(val_files, processor, sample_rate=SAMPLE_RATE, max_duration=MAX_DURATION_SEC)\n",
    "\n",
    "print(train_dataset[0])\n",
    "print(f\"훈련 데이터셋 샘플 수: {len(train_dataset)}\")\n",
    "print(f\"검증 데이터셋 샘플 수: {len(val_dataset)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "비어 있는 샘플 개수: 0\n"
     ]
    }
   ],
   "source": [
    "# only 디버깅용\n",
    "\n",
    "\n",
    "# 빈 샘플이 있는지 확인\n",
    "empty_samples = [i for i, sample in enumerate(train_dataset) if len(sample[\"input_values\"]) == 0 or sample[\"labels\"] == \"\"]\n",
    "print(f\"비어 있는 샘플 개수: {len(empty_samples)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 데이터 collator 정의 (dynamic padding 포함)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 데이터 collator 정의 (dynamic padding 포함)\n",
    "\n",
    "# # '비어 있는 샘플 개수: 0'라고 나오는데, 자꾸 파인튜닝 코드 실행 시 '배치의 샘플이 유효하지 않다'고 출력되어서 디버깅 코드 추가하였음\n",
    "\n",
    "# def data_collator(batch):\n",
    "#     # 배치가 딕셔너리 형태이면 이미 collate된 상태임\n",
    "#     if isinstance(batch, dict):\n",
    "#         speeches = batch.get(\"speech\", [])\n",
    "#         texts = batch.get(\"text\", [])\n",
    "#     else:\n",
    "#         # 리스트인 경우 개별 샘플을 필터링\n",
    "#         valid_batch = [\n",
    "#             item for item in batch \n",
    "#             if item and \"speech\" in item and \"text\" in item and len(item[\"speech\"]) > 0 and item[\"text\"]\n",
    "#         ]\n",
    "#         if len(valid_batch) == 0:\n",
    "#             raise ValueError(\"모든 배치의 샘플이 유효하지 않습니다.\")\n",
    "#         speeches = [item[\"speech\"] for item in valid_batch]\n",
    "#         texts = [item[\"text\"] for item in valid_batch]\n",
    "\n",
    "#     print(f\"📌 최종 배치 크기: {len(speeches)}\")\n",
    "#     print(f\" 첫 번째 wav 길이: {len(speeches[0]) if speeches else 'N/A'}\")\n",
    "#     print(f\" 첫 번째 텍스트: {texts[0] if texts else 'N/A'}\")\n",
    "    \n",
    "#     inputs = processor(speeches, text=texts, sampling_rate=SAMPLE_RATE, padding=True, return_tensors=\"pt\")\n",
    "#     return inputs\n",
    "\n",
    "\n",
    "\n",
    "# print(\"data_collator 정의 완료\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_collator(features):\n",
    "    # features는 {\"input_values\": ..., \"labels\": ...} 형태의 딕셔너리 리스트입니다.\n",
    "    if isinstance(features, list):\n",
    "        if features and isinstance(features[0], dict):\n",
    "            # \"input_values\" 키가 있는지 확인합니다.\n",
    "            if \"input_values\" in features[0]:\n",
    "                input_values = [f[\"input_values\"] for f in features]\n",
    "                labels = [f[\"labels\"] for f in features]\n",
    "            else:\n",
    "                raise ValueError(\"키 'input_values'가 존재하지 않습니다.\")\n",
    "        else:\n",
    "            raise ValueError(\"리스트 내 요소가 dict 형태가 아닙니다.\")\n",
    "    elif isinstance(features, dict):\n",
    "        if \"input_values\" in features:\n",
    "            input_values = features.get(\"input_values\", [])\n",
    "            labels = features.get(\"labels\", [])\n",
    "        else:\n",
    "            raise ValueError(\"dict 형태지만 'input_values' 키가 없습니다.\")\n",
    "    else:\n",
    "        raise ValueError(\"배치의 형식이 올바르지 않습니다.\")\n",
    "    \n",
    "    # processor를 사용해 동적 패딩 수행\n",
    "    batch = processor(input_values, text=labels, sampling_rate=SAMPLE_RATE, padding=True, return_tensors=\"pt\")\n",
    "    return batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델, 프로세서 로딩 이후에 gradient checkpointing 활성화\n",
    "model.gradient_checkpointing_enable()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training aruments 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TrainingArguments 설정 완료\n"
     ]
    }
   ],
   "source": [
    "# Training arguments 설정정\n",
    "\n",
    "# 하이퍼파라미터 설정은 위에서 하기 \n",
    "training_args = TrainingArguments(\n",
    "    output_dir=MODEL_SAVE_DIR,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=BATCH_SIZE,\n",
    "    gradient_accumulation_steps=GRAD_ACCUMULATION_STEPS,\n",
    "    \n",
    "    eval_strategy=\"epoch\", # evaluation_strategy는 구버젼에서만 잘됨\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    num_train_epochs=NUM_EPOCHS,\n",
    "    warmup_ratio=WARMUP_RATIO,\n",
    "    fp16=True, # mixed precision\n",
    "    logging_steps=100,\n",
    "    save_total_limit=2,  # best 결과 + latest 결과만 저장\n",
    "    load_best_model_at_end=True,\n",
    "    dataloader_num_workers=0\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "print(\"TrainingArguments 설정 완료\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 평가 지표 함수 정의 (WER 기반)\n",
    "evaluation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluation metric 함수 정의 완료\n"
     ]
    }
   ],
   "source": [
    "# 평가 지표 함수 정의 (WER 기반)\n",
    "\n",
    "wer_metric = evaluate.load(\"wer\") # evaluate 라이브러리를 사용해 WER metric 로드\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    \"\"\"\n",
    "        예측 결과와 라벨을 processor를 사용하여 디코딩 후 WER 계산\n",
    "    \"\"\"\n",
    "    pred_logits = pred.predictions\n",
    "    pred_ids = np.argmax(pred_logits, axis=-1)\n",
    "    pred_str = processor.batch_decode(pred_ids)\n",
    "    \n",
    "    # -100은 무시 처리: label_ids 내의 값들을 디코딩 \n",
    "    label_ids = pred.label_ids\n",
    "    label_str = processor.batch_decode(label_ids, group_tokens=False) # group_tokens=False 옵션이 뭔지 잘 모름\n",
    "    \n",
    "    wer = wer_metric.compute(predictions=pred_str, references=label_str) # prediction vs label\n",
    "    return {\"wer\": wer}\n",
    "\n",
    "print(\"evaluation metric 함수 정의 완료\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "첫 번째 샘플 speech 길이: 282633\n",
      "첫 번째 샘플 text: 낮고 그다음에 프롭의 추진력이 좀 세야 되 강해야 되기 때문에 코드가 긴 (sf)/(에스에프) 형상 이런 형상을 기준으로 선 선택하는게 좀 더 좋고 저중량 같은 경우에는 코드가 좀 짧은 이런 형상을 선택하는 것이 조금 더 좋을 수 있다 볼 수 있는 거예요.\n"
     ]
    }
   ],
   "source": [
    "# # 데이터셋 확인\n",
    "# for i in range(len(train_dataset)):\n",
    "#     sample = train_dataset[i]\n",
    "#     if sample is None:\n",
    "#         print(f\"인덱스 {i}는 유효하지 않습니다.\")\n",
    "\n",
    "\n",
    "# empty_samples = [i for i, sample in enumerate(train_dataset) if not sample[\"speech\"] or not sample[\"text\"]]\n",
    "# print(f\"비어 있는 샘플 개수: {len(empty_samples)}\")\n",
    "\n",
    "sample = train_dataset[0]\n",
    "print(f\"첫 번째 샘플 speech 길이: {len(sample['input_values'])}\")\n",
    "print(f\"첫 번째 샘플 text: {sample['labels']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trainer 객체 생성  & 학습 실행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainer 객체 생성 완료\n",
      "파인튜닝 시작\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='6183' max='22325' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 6183/22325 4:54:24 < 12:48:52, 0.35 it/s, Epoch 0.28/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[23]\u001b[39m\u001b[32m, line 19\u001b[39m\n\u001b[32m     15\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mTrainer 객체 생성 완료\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     16\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m파인튜닝 시작\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m19\u001b[39m train_result = \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     20\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m파인튜닝 완료\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/venvs/seungminENV_pip/lib/python3.12/site-packages/transformers/trainer.py:2241\u001b[39m, in \u001b[36mTrainer.train\u001b[39m\u001b[34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[39m\n\u001b[32m   2239\u001b[39m         hf_hub_utils.enable_progress_bars()\n\u001b[32m   2240\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2241\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2242\u001b[39m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2243\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2244\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2245\u001b[39m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2246\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/venvs/seungminENV_pip/lib/python3.12/site-packages/transformers/trainer.py:2548\u001b[39m, in \u001b[36mTrainer._inner_training_loop\u001b[39m\u001b[34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[39m\n\u001b[32m   2541\u001b[39m context = (\n\u001b[32m   2542\u001b[39m     functools.partial(\u001b[38;5;28mself\u001b[39m.accelerator.no_sync, model=model)\n\u001b[32m   2543\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m i != \u001b[38;5;28mlen\u001b[39m(batch_samples) - \u001b[32m1\u001b[39m\n\u001b[32m   2544\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.accelerator.distributed_type != DistributedType.DEEPSPEED\n\u001b[32m   2545\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m contextlib.nullcontext\n\u001b[32m   2546\u001b[39m )\n\u001b[32m   2547\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[32m-> \u001b[39m\u001b[32m2548\u001b[39m     tr_loss_step = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2550\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m   2551\u001b[39m     args.logging_nan_inf_filter\n\u001b[32m   2552\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[32m   2553\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m (torch.isnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch.isinf(tr_loss_step))\n\u001b[32m   2554\u001b[39m ):\n\u001b[32m   2555\u001b[39m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[32m   2556\u001b[39m     tr_loss = tr_loss + tr_loss / (\u001b[32m1\u001b[39m + \u001b[38;5;28mself\u001b[39m.state.global_step - \u001b[38;5;28mself\u001b[39m._globalstep_last_logged)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/venvs/seungminENV_pip/lib/python3.12/site-packages/transformers/trainer.py:3740\u001b[39m, in \u001b[36mTrainer.training_step\u001b[39m\u001b[34m(***failed resolving arguments***)\u001b[39m\n\u001b[32m   3737\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.accelerator.distributed_type == DistributedType.DEEPSPEED:\n\u001b[32m   3738\u001b[39m     kwargs[\u001b[33m\"\u001b[39m\u001b[33mscale_wrt_gas\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m3740\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43maccelerator\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3742\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m loss.detach()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/venvs/seungminENV_pip/lib/python3.12/site-packages/accelerate/accelerator.py:2325\u001b[39m, in \u001b[36mAccelerator.backward\u001b[39m\u001b[34m(self, loss, **kwargs)\u001b[39m\n\u001b[32m   2323\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m   2324\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.scaler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2325\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mscaler\u001b[49m\u001b[43m.\u001b[49m\u001b[43mscale\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2326\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m learning_rate \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.has_lomo_optimizer:\n\u001b[32m   2327\u001b[39m     \u001b[38;5;28mself\u001b[39m.lomo_backward(loss, learning_rate)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/venvs/seungminENV_pip/lib/python3.12/site-packages/torch/_tensor.py:626\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    616\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    617\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    618\u001b[39m         Tensor.backward,\n\u001b[32m    619\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    624\u001b[39m         inputs=inputs,\n\u001b[32m    625\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m626\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    627\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    628\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/venvs/seungminENV_pip/lib/python3.12/site-packages/torch/autograd/__init__.py:347\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    342\u001b[39m     retain_graph = create_graph\n\u001b[32m    344\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    345\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    346\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m347\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    348\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    349\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    350\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    351\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    352\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    353\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    354\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/venvs/seungminENV_pip/lib/python3.12/site-packages/torch/autograd/graph.py:823\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    821\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    822\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m823\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    824\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    825\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    826\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    827\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m현재 셀 또는 이전 셀에서 코드를 실행하는 동안 Kernel이 충돌했습니다. \n",
      "\u001b[1;31m셀의 코드를 검토하여 가능한 오류 원인을 식별하세요. \n",
      "\u001b[1;31m자세한 내용을 보려면 <a href='https://aka.ms/vscodeJupyterKernelCrash'>여기</a>를 클릭하세요. \n",
      "\u001b[1;31m자세한 내용은 Jupyter <a href='command:jupyter.viewOutput'>로그</a>를 참조하세요."
     ]
    }
   ],
   "source": [
    "# Trainer 객체 생성  & 학습 실행\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset\n",
    "    # tokenizer=processor\n",
    "    # dataloader_pin_memory=False # 최신버젼에서는 미지원이라\n",
    "    # remove_unused_columns=False\n",
    ")\n",
    "\n",
    "print(\"Trainer 객체 생성 완료\")\n",
    "print(\"파인튜닝 시작\")\n",
    "\n",
    "\n",
    "train_result = trainer.train()\n",
    "print(\"파인튜닝 완료\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 모델 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 저장\n",
    "\n",
    "# 최종 모델과 프로세서 저장 (.pt 형식은 아니지만 Hugging Face 포맷으로 저장 후 변환 가능)\n",
    "model.save_pretrained(MODEL_SAVE_DIR)\n",
    "processor.save_pretrained(MODEL_SAVE_DIR)\n",
    "\n",
    "print(f\"모델이 {MODEL_SAVE_DIR}에 저장됨\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "seungminENV_pip",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
