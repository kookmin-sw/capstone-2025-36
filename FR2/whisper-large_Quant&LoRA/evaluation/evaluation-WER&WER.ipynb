{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "146a433a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3e6139f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d4671062df94d20980ea1ad4a670ee1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1382 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18e4e74fec3040e2b468df6e2d4e64ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_model.safetensors:   0%|          | 0.00/63.0M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/173 [00:00<?, ?it/s]/tmp/ipykernel_538260/3072454204.py:133: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "100%|██████████| 173/173 [40:13<00:00, 13.95s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ WER: 30.72%\n",
      "✅ CER: 10.03%\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m현재 셀 또는 이전 셀에서 코드를 실행하는 동안 Kernel이 충돌했습니다. \n",
      "\u001b[1;31m셀의 코드를 검토하여 가능한 오류 원인을 식별하세요. \n",
      "\u001b[1;31m자세한 내용을 보려면 <a href='https://aka.ms/vscodeJupyterKernelCrash'>여기</a>를 클릭하세요. \n",
      "\u001b[1;31m자세한 내용은 Jupyter <a href='command:jupyter.viewOutput'>로그</a>를 참조하세요."
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "import numpy as np\n",
    "import gc\n",
    "from dataclasses import dataclass\n",
    "from typing import Any, Dict, List, Union\n",
    "from tqdm import tqdm\n",
    "from datasets import Dataset, DatasetDict, Audio\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# 데이터셋 불러오기 함수 (wav 파일과 대응하는 txt 또는 json 파일에서 transcript 읽기)\n",
    "def load_my_dataset(folder_path):\n",
    "    audio_paths = []\n",
    "    transcripts = []\n",
    "\n",
    "    for fname in sorted(os.listdir(folder_path)):\n",
    "        if fname.endswith(\".wav\"):\n",
    "            wav_path = os.path.join(folder_path, fname)\n",
    "            base_name = fname.replace(\".wav\", \"\")\n",
    "            txt_path = os.path.join(folder_path, base_name + \".txt\")\n",
    "            json_path = os.path.join(folder_path, base_name + \".json\")\n",
    "\n",
    "            # txt 파일이 있으면 우선 읽고, 없으면 json에서 읽음\n",
    "            if os.path.exists(txt_path):\n",
    "                with open(txt_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                    text = f.read().strip()\n",
    "            elif os.path.exists(json_path):\n",
    "                with open(json_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                    j = json.load(f)\n",
    "                    text = j[\"06_transcription\"][\"1_text\"]\n",
    "            else:\n",
    "                continue\n",
    "\n",
    "            audio_paths.append(wav_path)\n",
    "            transcripts.append(text)\n",
    "\n",
    "    return Dataset.from_dict({\n",
    "        \"audio\": audio_paths,\n",
    "        \"sentence\": transcripts,\n",
    "    })\n",
    "\n",
    "# 데이터셋 로드 및 전처리\n",
    "# test 데이터셋 경로 \n",
    "common_voice = DatasetDict()\n",
    "# common_voice[\"test\"] = load_my_dataset(\"/data/seungmin/dataset/kr_univ_validation_processed/number_and_english/\")\n",
    "common_voice[\"test\"] = load_my_dataset(\"/data/seungmin/dataset/k12_validation_processed/number_and_english/\")\n",
    "\n",
    "# 샘플링레이트 16kHz로 강제 변환\n",
    "common_voice[\"test\"] = common_voice[\"test\"].cast_column(\"audio\", Audio(sampling_rate=16000))\n",
    "\n",
    "# Whisper 관련 모듈 불러오기 (원본 base 모델 경로와 언어, task 설정)\n",
    "from transformers import WhisperFeatureExtractor, WhisperTokenizer, WhisperProcessor\n",
    "\n",
    "model_name_or_path = \"openai/whisper-large-v2\"  # base 모델 명시\n",
    "language = \"korean\"\n",
    "task = \"transcribe\"\n",
    "\n",
    "# feature extractor, tokenizer, processor 로드\n",
    "feature_extractor = WhisperFeatureExtractor.from_pretrained(model_name_or_path)\n",
    "tokenizer = WhisperTokenizer.from_pretrained(model_name_or_path, language=language, task=task)\n",
    "processor = WhisperProcessor.from_pretrained(model_name_or_path, language=language, task=task)\n",
    "\n",
    "# 데이터 전처리 함수: 입력 오디오로부터 input_features 생성 및 텍스트를 label id로 인코딩\n",
    "def prepare_dataset(batch):\n",
    "    audio = batch[\"audio\"]\n",
    "    batch[\"input_features\"] = feature_extractor(audio[\"array\"], sampling_rate=audio[\"sampling_rate\"]).input_features[0]\n",
    "    batch[\"labels\"] = tokenizer(batch[\"sentence\"]).input_ids\n",
    "    return batch\n",
    "\n",
    "common_voice[\"test\"] = common_voice[\"test\"].map(\n",
    "    prepare_dataset,\n",
    "    remove_columns=common_voice[\"test\"].column_names,\n",
    "    num_proc=1\n",
    ")\n",
    "\n",
    "# Data Collator: 입력과 라벨에 대해 각각 적절한 패딩 수행\n",
    "@dataclass\n",
    "class DataCollatorSpeechSeq2SeqWithPadding:\n",
    "    processor: Any\n",
    "\n",
    "    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
    "        # 오디오 입력 패딩\n",
    "        input_features = [{\"input_features\": feature[\"input_features\"]} for feature in features]\n",
    "        batch = self.processor.feature_extractor.pad(input_features, return_tensors=\"pt\")\n",
    "        \n",
    "        # 라벨(tokenized text) 패딩\n",
    "        label_features = [{\"input_ids\": feature[\"labels\"]} for feature in features]\n",
    "        labels_batch = self.processor.tokenizer.pad(label_features, return_tensors=\"pt\")\n",
    "        labels = labels_batch[\"input_ids\"].masked_fill(labels_batch.attention_mask.ne(1), -100)\n",
    "        \n",
    "        # 만약 bos 토큰이 앞에 추가된 경우 제거 (모델 generate 단계에서 다시 추가됨)\n",
    "        if (labels[:, 0] == self.processor.tokenizer.bos_token_id).all().cpu().item():\n",
    "            labels = labels[:, 1:]\n",
    "        batch[\"labels\"] = labels\n",
    "\n",
    "        return batch\n",
    "\n",
    "data_collator = DataCollatorSpeechSeq2SeqWithPadding(processor=processor)\n",
    "\n",
    "# 평가를 위한 DataLoader 구성 (배치 사이즈 등 필요에 따라 조정)\n",
    "eval_dataloader = DataLoader(common_voice[\"test\"], batch_size=8, collate_fn=data_collator)\n",
    "\n",
    "# 허깅페이스에 업로드한 PEFT 모델 로드\n",
    "from transformers import WhisperForConditionalGeneration, BitsAndBytesConfig\n",
    "from peft import PeftModel, PeftConfig\n",
    "\n",
    "# 업로드 시 사용한 모델 아이디 (예: handsomemin/openai-whisper-large-v2-k12-0416-LORA)\n",
    "peft_model_id = \"handsomemin/openai-whisper-large-v2-k12-0416-LORA-LORA\"\n",
    "peft_config = PeftConfig.from_pretrained(peft_model_id)\n",
    "\n",
    "# base 모델을 8bit 로딩하고, device_map=\"auto\"를 사용하여 GPU에 자동 매핑\n",
    "model = WhisperForConditionalGeneration.from_pretrained(\n",
    "    peft_config.base_model_name_or_path,\n",
    "    quantization_config=BitsAndBytesConfig(load_in_8bit=True),\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "# 저장된 adapter를 로드하여 PEFT 모델로 완성\n",
    "model = PeftModel.from_pretrained(model, peft_model_id)\n",
    "model.config.forced_decoder_ids = None\n",
    "model.config.suppress_tokens = []\n",
    "model.eval()\n",
    "\n",
    "# 평가 지표 준비 (wer, cer)\n",
    "from evaluate import load\n",
    "wer_metric = load(\"wer\")\n",
    "cer_metric = load(\"cer\")\n",
    "\n",
    "# 평가 루프: 생성된 결과와 정답 텍스트를 디코딩하여 평가 지표에 추가\n",
    "for step, batch in enumerate(tqdm(eval_dataloader)):\n",
    "    # GPU 메모리 절약과 속도를 위해 autocast 사용 (FP16)\n",
    "    with torch.cuda.amp.autocast():\n",
    "        with torch.no_grad():\n",
    "            # decoder_input_ids는 간단히 토큰의 앞부분을 사용\n",
    "            generated_tokens = model.generate(\n",
    "                input_features=batch[\"input_features\"].to(\"cuda\"),\n",
    "                decoder_input_ids=batch[\"labels\"][:, :4].to(\"cuda\"),\n",
    "                attention_mask=torch.ones_like(batch[\"input_features\"]).to(\"cuda\"),\n",
    "                max_new_tokens=255,\n",
    "            ).cpu().numpy()\n",
    "\n",
    "    # 라벨 처리: -100을 tokenizer.pad_token_id로 바꾸어 디코딩\n",
    "    labels = batch[\"labels\"].cpu().numpy()\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_preds = tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    wer_metric.add_batch(predictions=decoded_preds, references=decoded_labels)\n",
    "    cer_metric.add_batch(predictions=decoded_preds, references=decoded_labels)\n",
    "\n",
    "    # 메모리 정리\n",
    "    del generated_tokens, labels, batch\n",
    "    gc.collect()\n",
    "\n",
    "# 평가 지표 계산 및 출력 (백분율 단위)\n",
    "wer = 100 * wer_metric.compute()\n",
    "cer = 100 * cer_metric.compute()\n",
    "print(f\"WER: {wer:.2f}%\")\n",
    "print(f\"CER: {cer:.2f}%\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "seungmin_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
