{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Œ í•œê¸€ ìëª¨(ã„±~ã…, ã…~ã…£)ê°€ í¬í•¨ëœ JSON íŒŒì¼ ê°œìˆ˜: 0\n",
      "ğŸ” ë°œê²¬ëœ í•œê¸€ ìëª¨ ëª©ë¡: set()\n",
      "ğŸ“ ìƒ˜í”Œ JSON íŒŒì¼ ëª©ë¡: []\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "\n",
    "# ë°ì´í„°ì…‹ ê²½ë¡œ\n",
    "data_dirs = [\n",
    "    \"/data/seungmin/dataset/kr_univ_training_processed/number_and_english\",\n",
    "    \"/data/seungmin/dataset/kr_univ_training_processed/number_or_english\",\n",
    "    \"/data/seungmin/dataset/kr_univ_validation_processed/number_and_english\",\n",
    "    \"/data/seungmin/dataset/kr_univ_validation_processed/number_or_english\"\n",
    "]\n",
    "\n",
    "# í•œê¸€ ìëª¨ ê²€ì¶œ í•¨ìˆ˜\n",
    "def has_korean_jamo(text):\n",
    "    return bool(re.search(r\"[ã„±-ã…ã…-ã…£]\", text))\n",
    "\n",
    "# í•œê¸€ ìëª¨ê°€ í¬í•¨ëœ íŒŒì¼ ë¦¬ìŠ¤íŠ¸\n",
    "files_with_jamo = []\n",
    "detected_jamo_set = set()\n",
    "\n",
    "for data_dir in data_dirs:\n",
    "    for root, _, files in os.walk(data_dir):\n",
    "        for file in files:\n",
    "            if file.endswith('.json') and not file.startswith('._'):\n",
    "                json_file_path = os.path.join(root, file)\n",
    "\n",
    "                # íŒŒì¼ í¬ê¸° í™•ì¸ (ë¹ˆ íŒŒì¼ ê±´ë„ˆë›°ê¸°)\n",
    "                if os.path.getsize(json_file_path) == 0:\n",
    "                    print(f\" ë¹ˆ íŒŒì¼ ë°œê²¬: {json_file_path}\")\n",
    "                    continue\n",
    "\n",
    "                try:\n",
    "                    with open(json_file_path, 'r', encoding='utf-8') as f:\n",
    "                        raw_data = f.read()\n",
    "\n",
    "                        # JSON ë‚´ìš©ì´ ë¹„ì–´ìˆëŠ” ê²½ìš° ê±´ë„ˆë›°ê¸°\n",
    "                        if not raw_data.strip():\n",
    "                            print(f\"JSON ë‚´ìš© ì—†ìŒ: {json_file_path}\")\n",
    "                            continue\n",
    "\n",
    "                        # JSON íŒŒì‹±\n",
    "                        data = json.loads(raw_data)\n",
    "\n",
    "                except json.JSONDecodeError as e:\n",
    "                    print(f\" -------------------------JSON ì˜¤ë¥˜: {json_file_path} | {e}-----------------\")\n",
    "                    continue\n",
    "\n",
    "                # í•œê¸€ ìëª¨ í¬í•¨ ì—¬ë¶€ í™•ì¸\n",
    "                if \"06_transcription\" in data and \"1_text\" in data[\"06_transcription\"]:\n",
    "                    text = data[\"06_transcription\"][\"1_text\"].strip()\n",
    "                    if has_korean_jamo(text):\n",
    "                        files_with_jamo.append(json_file_path)\n",
    "                        detected_jamo_set.update(re.findall(r\"[ã„±-ã…ã…-ã…£]\", text))\n",
    "\n",
    "# ê²°ê³¼ ì¶œë ¥\n",
    "print(\"í•œê¸€ ìëª¨(ã„±~ã…, ã…~ã…£)ê°€ í¬í•¨ëœ JSON íŒŒì¼ ê°œìˆ˜:\", len(files_with_jamo))\n",
    "print(\"ë°œê²¬ëœ í•œê¸€ ìëª¨ ëª©ë¡:\", detected_jamo_set)\n",
    "print(\"ìƒ˜í”Œ JSON íŒŒì¼ ëª©ë¡:\", files_with_jamo[:5])  # ì¼ë¶€ ìƒ˜í”Œë§Œ ì¶œë ¥\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Œ ì¼ë°˜ì ì¸ íŠ¹ìˆ˜ê¸°í˜¸ê°€ í¬í•¨ëœ JSON íŒŒì¼ ê°œìˆ˜: 2100\n",
      "ğŸ” ë°œê²¬ëœ íŠ¹ìˆ˜ê¸°í˜¸ ëª©ë¡: {'ã¡', '$', 'Î”', 'ã', 'Â²', 'ğ‘¨', 'Ã·', 'âˆš', 'Ïƒ', 'ï¼', 'Ã—', '\\\\', '>', 'Ï‰', 'Î¸', 'ğ¿', '%', 'âˆ', 'Ï„', '-', 'ï¼‹', '#', 'â„ƒ', 'ãœ', 'Âº', 'Ëš', 'â‚¬', 'Ï€', 'Â°', 'Î¼', 'â€“', 'ğ‘ ', 'ã¥', 'â–³', '(', 'Î´', '_', ')', '`', '&', '~'}\n",
      "ğŸ“ ìƒ˜í”Œ JSON íŒŒì¼ ëª©ë¡: ['/data/seungmin/dataset/kr_univ_training_processed/number_and_english/arch_C03105_U00678.json', '/data/seungmin/dataset/kr_univ_training_processed/number_and_english/arch_C02299_U00488.json', '/data/seungmin/dataset/kr_univ_training_processed/number_and_english/arch_C06621_U00502.json', '/data/seungmin/dataset/kr_univ_training_processed/number_and_english/arch_C03105_U00660.json', '/data/seungmin/dataset/kr_univ_training_processed/number_and_english/arch_C03971_U00113.json']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "\n",
    "# ë°ì´í„°ì…‹ ê²½ë¡œ\n",
    "data_dirs = [\n",
    "    \"/data/seungmin/dataset/kr_univ_training_processed/number_and_english\",\n",
    "    \"/data/seungmin/dataset/kr_univ_training_processed/number_or_english\",\n",
    "    \"/data/seungmin/dataset/kr_univ_validation_processed/number_and_english\",\n",
    "    \"/data/seungmin/dataset/kr_univ_validation_processed/number_or_english\"\n",
    "]\n",
    "\n",
    "# ìœ ì§€í•  ê¸°í˜¸ ëª©ë¡ (ìˆ˜í•™ ê¸°í˜¸ í¬í•¨)\n",
    "allowed_symbols = set(\"=.,?!\")\n",
    "\n",
    "# íŠ¹ìˆ˜ê¸°í˜¸ ê²€ì¶œ í•¨ìˆ˜\n",
    "def has_unwanted_symbols(text):\n",
    "    unwanted_chars = re.findall(r\"[^A-Za-z0-9ê°€-í£\\s=.,?!]\", text)\n",
    "    return set(unwanted_chars) if unwanted_chars else None\n",
    "\n",
    "# íŠ¹ìˆ˜ê¸°í˜¸ê°€ í¬í•¨ëœ íŒŒì¼ ë¦¬ìŠ¤íŠ¸\n",
    "files_with_symbols = []\n",
    "detected_symbols_set = set()\n",
    "\n",
    "for data_dir in data_dirs:\n",
    "    for root, _, files in os.walk(data_dir):\n",
    "        for file in files:\n",
    "            if file.endswith('.json') and not file.startswith('._'):\n",
    "                json_file_path = os.path.join(root, file)\n",
    "\n",
    "                with open(json_file_path, 'r', encoding='utf-8') as f:\n",
    "                    data = json.load(f)\n",
    "\n",
    "                if \"06_transcription\" in data and \"1_text\" in data[\"06_transcription\"]:\n",
    "                    text = data[\"06_transcription\"][\"1_text\"].strip()\n",
    "\n",
    "                    symbols = has_unwanted_symbols(text)\n",
    "                    if symbols:\n",
    "                        files_with_symbols.append(json_file_path)\n",
    "                        detected_symbols_set.update(symbols)\n",
    "\n",
    "# ê²°ê³¼ ì¶œë ¥\n",
    "print(\"ì¼ë°˜ì ì¸ íŠ¹ìˆ˜ê¸°í˜¸ê°€ í¬í•¨ëœ JSON íŒŒì¼ ê°œìˆ˜:\", len(files_with_symbols))\n",
    "print(\"ë°œê²¬ëœ íŠ¹ìˆ˜ê¸°í˜¸ ëª©ë¡:\", detected_symbols_set)\n",
    "print(\"ìƒ˜í”Œ JSON íŒŒì¼ ëª©ë¡:\", files_with_symbols[:5])  # ì¼ë¶€ ìƒ˜í”Œë§Œ ì¶œë ¥\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Œ '~' ê¸°í˜¸ê°€ í¬í•¨ëœ JSON íŒŒì¼ ê°œìˆ˜: 81\n",
      "ğŸ“‚ ê²°ê³¼ ì €ì¥: symbol_files.json\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "\n",
    "# ë°ì´í„°ì…‹ ê²½ë¡œ\n",
    "data_dirs = [\n",
    "    \"/data/seungmin/dataset/kr_univ_training_processed/number_and_english\",\n",
    "    \"/data/seungmin/dataset/kr_univ_training_processed/number_or_english\",\n",
    "    \"/data/seungmin/dataset/kr_univ_validation_processed/number_and_english\",\n",
    "    \"/data/seungmin/dataset/kr_univ_validation_processed/number_or_english\"\n",
    "]\n",
    "\n",
    "# ì°¾ê³  ì‹¶ì€ íŠ¹ìˆ˜ê¸°í˜¸ ì…ë ¥ (ì˜ˆ: 'Ï€' ë˜ëŠ” 'Â°' ë“±)\n",
    "target_symbol = \"~\"  # ì›í•˜ëŠ” ê¸°í˜¸ë¡œ ë³€ê²½\n",
    "\n",
    "# ìœ ì§€í•  ê¸°í˜¸ ëª©ë¡ (ìˆ˜í•™ ê¸°í˜¸ í¬í•¨)\n",
    "allowed_symbols = set(\"=.,?!\")\n",
    "\n",
    "# íŠ¹ìˆ˜ê¸°í˜¸ ê²€ì¶œ í•¨ìˆ˜\n",
    "def extract_unwanted_symbols(text):\n",
    "    return set(re.findall(r\"[^A-Za-z0-9ê°€-í£\\s=.,?!]\", text))\n",
    "\n",
    "# íŠ¹ìˆ˜ê¸°í˜¸ê°€ í¬í•¨ëœ íŒŒì¼ ë”•ì…”ë„ˆë¦¬ (ê¸°í˜¸ë³„ë¡œ ë§¤í•‘)\n",
    "files_with_symbols = {}\n",
    "detected_symbols_set = set()\n",
    "\n",
    "for data_dir in data_dirs:\n",
    "    for root, _, files in os.walk(data_dir):\n",
    "        for file in files:\n",
    "            if file.endswith('.json') and not file.startswith('._'):\n",
    "                json_file_path = os.path.join(root, file)\n",
    "\n",
    "                try:\n",
    "                    with open(json_file_path, 'r', encoding='utf-8') as f:\n",
    "                        data = json.load(f)\n",
    "\n",
    "                    if \"06_transcription\" in data and \"1_text\" in data[\"06_transcription\"]:\n",
    "                        text = data[\"06_transcription\"][\"1_text\"].strip()\n",
    "                        symbols = extract_unwanted_symbols(text)\n",
    "\n",
    "                        if symbols:\n",
    "                            detected_symbols_set.update(symbols)\n",
    "                            for symbol in symbols:\n",
    "                                if symbol not in files_with_symbols:\n",
    "                                    files_with_symbols[symbol] = []\n",
    "                                files_with_symbols[symbol].append(json_file_path)\n",
    "\n",
    "                except json.JSONDecodeError:\n",
    "                    print(f\"JSON íŒŒì¼ ì˜¤ë¥˜: {json_file_path}\")\n",
    "                except Exception as e:\n",
    "                    print(f\"íŒŒì¼ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {json_file_path} | {e}\")\n",
    "\n",
    "# íŠ¹ì • íŠ¹ìˆ˜ê¸°í˜¸ê°€ í¬í•¨ëœ íŒŒì¼ ì°¾ê¸°\n",
    "matched_files = files_with_symbols.get(target_symbol, [])\n",
    "\n",
    "# JSON íŒŒì¼ë¡œ ì €ì¥\n",
    "output_file = \"symbol_files.json\"\n",
    "with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(matched_files, f, indent=4, ensure_ascii=False)\n",
    "\n",
    "# ê²°ê³¼ ì¶œë ¥\n",
    "print(f\"'{target_symbol}' ê¸°í˜¸ê°€ í¬í•¨ëœ JSON íŒŒì¼ ê°œìˆ˜: {len(matched_files)}\")\n",
    "print(f\"ê²°ê³¼ ì €ì¥: {output_file}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "seungmin_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
